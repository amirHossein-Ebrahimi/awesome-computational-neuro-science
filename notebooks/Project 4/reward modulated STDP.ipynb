{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple\n",
    "# Disable warnings \n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_global_random_seed():\n",
    "    # fix seed cause reproducable result\n",
    "    seed = 1997118\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíÅ‚Äç‚ôÇÔ∏è getting help\n",
    "For **getting help** about parameters and return results:\n",
    "\n",
    "`help(function_you_dont_know)`  \n",
    "or use numpy info function   \n",
    "`np.info(function_you_dont_know)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Section\n",
    "\n",
    "- **Neuron**\n",
    "    - integrate\n",
    "    - generate_config\n",
    "- **Utils**\n",
    "    - clamp\n",
    "    - soft_bound_Ap\n",
    "    - soft_bound_An\n",
    "    - hard_bound\n",
    "    - soft_bound\n",
    "- **Connection**\n",
    "    - full alias for full_connectivity\n",
    "    - fixedNNP alias for fixed_number_of_presynaptics_parents_connectivity\n",
    "    - fixedCp alias for fixed_coupling_probability_connectivity\n",
    "    - set_relations\n",
    "- **Learning**\n",
    "    - stdp\n",
    "- **Population**\n",
    "    - activate\n",
    "    - transmit_spike\n",
    "    - check_activition\n",
    " \n",
    " Global\n",
    " - **step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\" Neuron Utility handlers \"\"\"\n",
    "    @staticmethod\n",
    "    def integrate(U, I, dt, config):\n",
    "        \"\"\"\n",
    "            Calculate next potentials list based on last potentials and Currents input which neurons get\n",
    "            Cause numpy brodcasting ability, functions may work correct for single entry for ndarrays \n",
    "\n",
    "            Keyword arguments:\n",
    "            U -- potentials array <numpy.ndarray>\n",
    "            I -- current stands for internat and extrenal currents which every neurons recives <numpy.ndarray>\n",
    "            dt -- time step for each step <float>\n",
    "            config -- A matrix containg configuration for each of neurons <numpy.ndarray>\n",
    "                      it must [u_rest, R, threshold, tau] it use array keyword referencing.\n",
    "            \n",
    "            return:\n",
    "            a tuple that contains (potentials, spikedActivity)\n",
    "            potentials -- new potentials calculated based on Leaky integrated and fire dynamics <numpu.ndarray>\n",
    "            spikedActivity -- boolean array which shows wheter neuron spiked in this step or not <numpy.ndarray>\n",
    "        \"\"\"\n",
    "        u_rest = config['u_rest']\n",
    "        potentials = U + (u_rest - U + config['R'] * I) * (dt / config['tau'])\n",
    "        spiked = potentials >= config['threshold']\n",
    "        potentials[spiked] = u_rest\n",
    "        return potentials, spiked\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_config(tau=5, resistor=5, threshold=-65, uRest=-70, size=None, noise=None):\n",
    "        \"\"\" \n",
    "            generate configuration for variables make them noisy in if args presented\n",
    "\n",
    "            Keyword arguments:\n",
    "            tau -- time constant for decays (default=5)\n",
    "            resistor -- (default=5)\n",
    "            threshold -- (default=-65)\n",
    "            uRest -- (default=-70)\n",
    "            size -- size of configuration (default=None)\n",
    "            noise -- setup for adding noise to configuration, make them not be exactly the same \n",
    "                    { size, mu, sigma } (default=None)\n",
    "                    mu -- mean of distribution (default=0)\n",
    "                    sigma -- standrad variation of distribution (default=1)\n",
    "\n",
    "            return: configurations\n",
    "\n",
    "            TODO: accept None in noise to prevent gauusain noise around specifi varibale\n",
    "            TODO: accpet diffrent configs for noise parameter, make it possible to have diffrent noise for diffrent configs \n",
    "            TODO: move noise to higger level use case, and keep pop homogenous \n",
    "            TODO: setup dtype optimization for lower memory allocation  \n",
    "        \"\"\"\n",
    "        if noise is None or size is None:\n",
    "            return np.array((tau, resistor, threshold, uRest), dtype=[('tau', 'f8'), ('R', 'f8'), ('threshold', 'f8'), ('u_rest', 'f8')])\n",
    "\n",
    "        def n(v): return v + np.random.normal(noise.get('mu', 0),noise.get('sigma', 1), size)\n",
    "        return {'tau': n(tau), \"R\": n(resistor), \"threshold\": n(threshold), 'u_rest': uRest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    @staticmethod\n",
    "    def clamp(value, minimum=None, maximum=None):\n",
    "        \"\"\"\n",
    "            Clamp entry value between specified bound, no clamp will be done if bound is not specifeid\n",
    "            Cause numpy brodcasting ability, functions may work correct for single entry for ndarrays\n",
    "            Do it inplace and will change entry value\n",
    "\n",
    "            Keyword arguments:\n",
    "            value -- Values which gonna be clamped <numpy.ndarray>\n",
    "            minimum -- lower bound (default=None)\n",
    "            maximum -- upper bound (default=None)\n",
    "            \n",
    "            updated value can be used\n",
    "        \"\"\"\n",
    "        if (minimum is not None):\n",
    "            value[value < minimum] = minimum\n",
    "        if (maximum is not None):\n",
    "            value[value > maximum] = maximum\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_bound_Ap(w, w_max=1, nu_positive=0.3):\n",
    "        \"\"\"\n",
    "            soft bound function for updating Weigths by A+(amplitude parameter) in stdp learnging\n",
    "\n",
    "            Keyword arguments:\n",
    "            w -- weights <numpy.ndarray>\n",
    "            w_max -- maximum value can w reach, it cause lower update in for close weigths (default=1)\n",
    "            nu_positive -- rating parameters for updaing weigths (deafult=0.3)\n",
    "\n",
    "            return: (w_max - w) * Œ∑+\n",
    "        \"\"\"\n",
    "        return (w_max - w) * nu_positive\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_bound_An(w,  w_min=0, nu_negative=0.2):\n",
    "        \"\"\"\n",
    "            soft bound function for updating Weigths by A-(amplitude parameter) in stdp learnging\n",
    "\n",
    "            Keyword arguments:\n",
    "            w -- weights <numpy.ndarray>\n",
    "            nu_negative -- rating parameters for updaing weigths (deafult=0.2)\n",
    "\n",
    "            return: (w - w_min) * Œ∑-\n",
    "        \"\"\"\n",
    "        return (w - w_min) * nu_negative\n",
    "\n",
    "    @staticmethod\n",
    "    def hard_bound(w, w_min, w_max):\n",
    "        \"\"\" Hard bound helper function for Weights \"\"\"\n",
    "        w[w < w_min] = 0\n",
    "        w[w > w_max] = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_bound(w, w_max, gamma, beta):\n",
    "        \"\"\" Soft bound helper function for Weights \"\"\"\n",
    "        return gamma * (w_max - w) ** beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connection:\n",
    "    \"\"\" \n",
    "        Connect population A to population B with connection_type strategy\n",
    "            full or full_connectivity,\n",
    "            fixedCP or fixed_coupling_probability_connectivity,\n",
    "            fixedNPP or fixed_number_of_presynaptics_parents,\n",
    "\n",
    "        Keyword arguments:\n",
    "        A -- size of pre-population <int>\n",
    "        B -- size of post-pospulation <int>\n",
    "        ------------------------------------\n",
    "        p -- coupling_probability (default=0.1)\n",
    "        J0 -- TODO:\n",
    "        r0 -- TODO:\n",
    "\n",
    "        TODO: For illiminate performance issue it doesn't connect neurons until calling @method for comp\n",
    "        TODO: Documentaion must be added in this case\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def full_connectivity(A, B, p=1, J0=10, r0=1, wMax=None, wMin=None):\n",
    "        pre_indexes, post_indexes = range(A), range(B)\n",
    "        C = p * A  # p * N\n",
    "        assert C != 0, 'Check size of your pre-poulation size, or increase p (No Devision by Zero)'\n",
    "        Œº, œÉ = J0/C, r0/math.sqrt(C)\n",
    "        normal = np.random.normal\n",
    "        relations = []\n",
    "        for pre in pre_indexes:\n",
    "            for post in post_indexes:\n",
    "                relations.append((pre, post, normal(Œº, œÉ)))\n",
    "        connections = Connection.set_relations(relations)\n",
    "        if (wMin is not None or wMax is not None):\n",
    "            Utils.clamp(connections['w'], wMin, wMax)\n",
    "        return connections\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def fixed_coupling_probability_connectivity(A, B, p=0.1, J0=10, r0=1, wMax=None, wMin=None):\n",
    "        pre_indexes, post_indexes = range(A), range(B)\n",
    "        CA = p * A\n",
    "        assert CA != 0, 'Check size of your pre-poulation size, or increase p (No Devision by Zero)'\n",
    "        CB = p * B\n",
    "        assert CB != 0, 'Check size of your post-poulation size, or increase p (No Devision by Zero)'\n",
    "        Œº, œÉ = J0/CB, r0/math.sqrt(CB)\n",
    "        normal = np.random.normal\n",
    "        relations = []\n",
    "        for pre in np.random.choice(pre_indexes, int(CA), replace=False):\n",
    "            for post in np.random.choice(post_indexes, int(CB), replace=False):\n",
    "                relations.append((pre, post, normal(Œº, œÉ)))\n",
    "        connections = Connection.set_relations(relations)\n",
    "        if (wMin is not None or wMax is not None):\n",
    "            Utils.clamp(connections['w'], wMin, wMax)\n",
    "        return connections\n",
    "\n",
    "    @staticmethod\n",
    "    def fixed_number_of_presynaptics_parents_connectivity(A, B, p=0.1, J0=10, r0=1, wMax=None, wMin=None):\n",
    "        pre_indexes, post_indexes = range(A), range(B)\n",
    "        C = p * A\n",
    "        assert C != 0, 'Check size of your pre-poulation size, or increase p (No Devision by Zero)'\n",
    "        Œº, œÉ = J0/C, r0/math.sqrt(C)\n",
    "        normal = np.random.normal\n",
    "        relations = []\n",
    "        for post in post_indexes:\n",
    "            for pre in np.random.choice(pre_indexes, int(C), replace=False):\n",
    "                relations.append((pre, post, normal(Œº, œÉ)))\n",
    "        connections = Connection.set_relations(relations)\n",
    "        if (wMin is not None or wMax is not None):\n",
    "            Utils.clamp(connections['w'], wMin, wMax)\n",
    "        return connections\n",
    "\n",
    "    @staticmethod\n",
    "    def set_relations(relations):\n",
    "        \"\"\"\n",
    "            Keyword arguments:\n",
    "            relations -- relation connections <iterator with (pre_index, post_index, weight_for_connection)>\n",
    "            return ndarray < pre, post, w >\n",
    "            pre -- index of pre synaptic neurons\n",
    "            post -- index of post synaptic neurons\n",
    "            w -- weight for connections\n",
    "        \"\"\"\n",
    "        return np.array(relations, dtype=[('pre', np.uint16), ('post', np.uint16), ('w', np.float64)])\n",
    "\n",
    "    # short hand for connectivities\n",
    "    full = full_connectivity.__func__\n",
    "    fixedCP = fixed_coupling_probability_connectivity.__func__\n",
    "    fixedNPP = fixed_number_of_presynaptics_parents_connectivity.__func__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning:\n",
    "    @staticmethod\n",
    "    def stdp(t_pre, t_post, W,  **config):\n",
    "        Œît = t_post - t_pre\n",
    "        Œîw = np.empty_like(Œît)\n",
    "        # https://books.google.co.uk/books?id=GCguDwAAQBAJ&pg=PA92&lpg=PA92&dq=stdp+algorithm+pseudocode&source=bl&ots=thYibFM0y3&sig=ACfU3U2vYzpdYZsEjpfC0nJB128J0U3GVw&hl=en&sa=X&redir_esc=y#v=onepage&q=stdp%20algorithm%20pseudocode&f=false\n",
    "\n",
    "        Œîtp = Œît >= 0\n",
    "        if np.any(Œîtp):\n",
    "            Ap = config.get('Ap', Utils.soft_bound_Ap)\n",
    "            œÑp = config.get('taup', 0.03)\n",
    "            Œ∑p = config.get('etap', 0.03)\n",
    "            w_max = config.get('wMax', 1)\n",
    "            Œîw[Œîtp] = Ap(W[Œîtp], w_max, Œ∑p) * np.exp(-Œît[Œîtp] / œÑp)\n",
    "\n",
    "        Œîtn = Œît < 0\n",
    "        if np.any(Œîtn):\n",
    "            An = config.get('An', Utils.soft_bound_An)\n",
    "            Œ∑n = config.get('etan', 0.02)\n",
    "            œÑn = config.get('taun', 0.02)\n",
    "            w_min = config.get('wMin', 0)\n",
    "            Œîw[Œîtn] = -An(W[Œîtn], w_min, Œ∑n) * np.exp(Œît[Œîtn] / œÑn)\n",
    "\n",
    "        Œîw[np.isnan(Œîw)] = 0\n",
    "        Œîw[np.isinf(Œîw)] = 0\n",
    "        Œît[np.isnan(Œît)] = -np.inf\n",
    "        return Œîw, Œît"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population:\n",
    "    \"\"\" \n",
    "        Gonna Handle homogenous population \n",
    "\n",
    "        Keyword argumetns:\n",
    "        neuron -- Neuronal model LIF is implemented see:Neuron class\n",
    "        neuron_configs -- dictionary that will spread to Neuron.generate_config function, see generate_config doc.\n",
    "        **config -- extra aruments such as \n",
    "            timeStep -- dt <float> (default=0.1)\n",
    "            hasInhibition -- <boolean> (deafult=False)\n",
    "            duration --duration of process used for generating time series for process <float> (default=5)\n",
    "            size -- number of neurons in this populations <int> (default=1000)\n",
    "    \"\"\"\n",
    "    def __init__(self, neuron, neuron_configs={}, **configs):\n",
    "        self.dt = configs.get('timeStep', 0.1)\n",
    "        duration = configs.get('duration', 5)\n",
    "        self.time_series = np.arange(0, duration, self.dt)\n",
    "        self.size = configs.get('size', 1000)\n",
    "        self.isi = (-1) ** int(configs.get('hasInhibition', False))\n",
    "\n",
    "        self.potentials_series = np.empty((self.size, self.time_series.size))\n",
    "        self.neuron_configs = Neuron.generate_config(\n",
    "            **neuron_configs, size=self.size)\n",
    "        self.last_spiked = np.ones(self.size)\n",
    "        self.last_spiked.fill(np.nan)\n",
    "\n",
    "        self.I_internal = np.zeros(self.size)\n",
    "        # minus epsilon for not firing in first time\n",
    "        self.potentials_series[:, 0] = self.neuron_configs['u_rest'] - 0.1\n",
    "\n",
    "    def activate(self, t, Iext=30.0):\n",
    "        \"\"\" \n",
    "            Activate a population for one step \n",
    "            \n",
    "            Keyword arguments:\n",
    "            t -- time index \n",
    "            Iext --  (deafult=30)\n",
    "\n",
    "            return: spiked_map\n",
    "            \n",
    "            TODO: need I_ext calculation from static fixed I ~ N(Œº, œÉ), or separated for each individual\n",
    "            TODO: I is now constant and not configured for pop (@critical) \n",
    "        \"\"\"\n",
    "        assert 1 <= t < self.time_series.size, 'Please Enter an integer belongs to [1, duration/dt) range as an index.'\n",
    "        assert isinstance(\n",
    "            Iext, float) or Iext.shape == self.I_internal.shape, 'Please Enter an integer belongs to [1, duration/dt) range as an index.'\n",
    "        I = self.I_internal + Iext\n",
    "        ut, spiked = Neuron.integrate(\n",
    "            self.potentials_series[:, t - 1], I, self.dt, self.neuron_configs)\n",
    "        self.last_spiked[spiked] = self.time_series[t]\n",
    "        self.potentials_series[:, t] = ut\n",
    "        return spiked\n",
    "\n",
    "    def transmit_spike(self, indices=None, weights=None, isi=1, decay=None):\n",
    "        \"\"\"\n",
    "            Transmit weights update effect to connected populations\n",
    "            Keyword arguments:\n",
    "            indeces -- indeces of neuron which going to update (deafult=None)\n",
    "            weights -- weights of connections related to indeces (deafult=None)\n",
    "            isi -- shows that updates are from inhibitory pop or excititory pop\n",
    "            decay -- amount of decay, usefull when reseting make learning time long\n",
    "\n",
    "            reset or reduce Tranmistion near to its initial zero where one of indeces or weights are not passed to function\n",
    "        \"\"\"\n",
    "        if indices is None or weights is None:\n",
    "            if decay is None:\n",
    "                self.I_internal = np.zeros(self.size)\n",
    "            else:\n",
    "                self.I_internal *= decay\n",
    "        else:\n",
    "            self.I_internal[indices] += isi * weights\n",
    "\n",
    "    def check_activition(self, I, t=-1):\n",
    "        return Neuron.integrate(self.potentials_series[:, t], self.I_internal+I, self.dt, self.neuron_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1Ô∏è‚É£) First Question ‚ùì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(A, B, connections, target, t, I, stdp_config, modulated, dopamin, trace=[], delay=0):\n",
    "    is_pre = target == 'pre'\n",
    "    # PRE-POST::PRE/POST-PRE::POST SYNAPTIC FIRING -> WEIGHT UPDATES\n",
    "    target_pop = A if is_pre else B\n",
    "    spikes = target_pop.activate(t, I)\n",
    "    activated_pop = np.where(spikes)[0]\n",
    "    if not activated_pop.size:\n",
    "        return\n",
    "    for idx in activated_pop:\n",
    "        involved = connections[connections[target] == idx]\n",
    "\n",
    "        dw, dt = Learning.stdp(\n",
    "            A.last_spiked[involved['pre']], B.last_spiked[involved['post']], involved['w'], **stdp_config)\n",
    "\n",
    "        c = modulated['c']\n",
    "        c += -c/modulated['tau_c']  * target_pop.dt\n",
    "        c[connections[target] == idx] += dw * target_pop.dt\n",
    "        ds = (c * dopamin if isinstance(dopamin, (int, float)) else c*dopamin) * target_pop.dt\n",
    "        connections['w'][connections[target] == idx] += ds[connections[target] == idx]\n",
    "\n",
    "        if delay != 0:\n",
    "            # PSP \n",
    "            update = target_pop.potentials_series[idx,t-delay] - target_pop.neuron_configs['u_rest']\n",
    "            connections['w'][connections[target] == idx] *= update\n",
    "            \n",
    "        Utils.hard_bound(connections['w'],stdp_config['wMin'], stdp_config['wMax'])\n",
    "        trace.append((t, dw, dt, connections['w'].copy(), target.upper(), dopamin.copy(), spikes))\n",
    "        if is_pre:\n",
    "            B.transmit_spike(involved['post'], connections[connections['pre'] == idx]['w'], A.isi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1Ô∏è‚É£) First Question ‚ùì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pattern(size=10):\n",
    "    pattern = np.zeros(size)\n",
    "    pattern[np.random.randint(0, size)] = 1\n",
    "    return pattern\n",
    "\n",
    "def get_I(pattern, nth, maximum_current=15000.0):\n",
    "    return np.where(pattern == nth, maximum_current, 0)\n",
    "\n",
    "\n",
    "\"\"\" delayed based current\n",
    "    ellipsis = t, d => u[t-d]-u_rest\n",
    "    W*ellipsis(curretnTime, delay)    \n",
    "\"\"\"\n",
    "def get_pattern_effect(pattern, src, dest, connections, t=-1):\n",
    "    accumulated_weight_sum = np.zeros(dest.size, dtype=np.float64)\n",
    "    for nth in range(int(max(pattern))):\n",
    "        # based on entry time (default last) \n",
    "        ut, spiked = src.check_activition(get_I(pattern, nth+1))\n",
    "        indices = np.arange(0, src.size, 1)[spiked]\n",
    "        for idx in indices:\n",
    "            involved = connections[connections['pre'] == idx]\n",
    "            accumulated_weight_sum[involved['post']] += involved['w'] * (dest.potentials_series[:, t]-dest.neuron_configs['u_rest'])\n",
    "    return accumulated_weight_sum\n",
    "\n",
    "def spike_plotter(x, y, xlabel, ylabel, title, c=None, ax=None, save=False):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)\n",
    "    ax.scatter(x, y, s=1, c=c)\n",
    "    if save:\n",
    "        ax.savefig(f'{title}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_punishment(post_spikes, dopamin, pattern, c0, c1):\n",
    "    # punish\n",
    "    if not post_spikes:\n",
    "        dopamin *= 0.9\n",
    "    \n",
    "    spikes = post_spikes[-1][-1]\n",
    "    is_post = post_spikes[-1][-3] == 'POST'\n",
    "    if is_post:\n",
    "        if pattern==10 and spikes[0] and not spikes[1]:\n",
    "            dopamin *= 1.1\n",
    "        else:\n",
    "            dopamin[c0] *= 0.9\n",
    "\n",
    "        if pattern==11 and spikes[1] and not spikes[0]:\n",
    "            dopamin *= 1.1\n",
    "        else:\n",
    "            dopamin[c1] *= 0.9\n",
    "    \n",
    "    if len(post_spikes) == 1:\n",
    "        return\n",
    "    \n",
    "    spikes = post_spikes[-2][-1]\n",
    "    is_post = post_spikes[-2][-3] == 'POST'\n",
    "\n",
    "    if is_post:\n",
    "        if pattern==10 and spikes[0] and not spikes[1]:\n",
    "            dopamin *= 1.1\n",
    "        else:\n",
    "            dopamin[c0] *= 0.9\n",
    "\n",
    "        if pattern==11 and spikes[1] and not spikes[0]:\n",
    "            dopamin *= 1.1\n",
    "        else:\n",
    "            dopamin[c1] *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_pattern  = np.array((2, 1, 0, 0, 0, 0, 0, 0, 1, 2))\n",
    "second_pattern = np.array((2, 1, 2, 1, 3, 1, 2, 1, 2, 1))\n",
    "\n",
    "size = {\"first\": 200, \"second\": 190, \"start\": 15, \"middle\": 200, \"end\": 0}\n",
    "learnable_patterns = [first_pattern for i in range(size['first'])]\n",
    "learnable_patterns.extend([second_pattern for i in range(size['second'])])\n",
    "learnable_patterns.extend([random_pattern() for i in range(size['middle'])])\n",
    "random.shuffle(learnable_patterns)\n",
    "patterns = [random_pattern() for i in range(size['start'])]\n",
    "patterns.extend(learnable_patterns)\n",
    "patterns.extend([random_pattern() for i in range(size['end'])])\n",
    "del learnable_patterns\n",
    "\n",
    "def pattern_type(pattern):\n",
    "    if np.all(pattern - first_pattern):\n",
    "        return 10\n",
    "    if np.all(pattern - second_pattern):\n",
    "        return 11\n",
    "    return int(max(pattern))\n",
    "\n",
    "\n",
    "def many2many(\n",
    "    src_size, \n",
    "    dest_size, \n",
    "    config = {'timeStep': 0.01},\n",
    "    stdp_config = {'wMax': 25, 'wMin': 0, 'taup': 8,'taun': 6, 'etap': 0.04, 'etan': 0.05},\n",
    "    src_neuron_configs = {'tau': 5, 'resistor': 5,'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 2}},\n",
    "    dest_neuron_configs = {'tau': 5, 'resistor': 5, 'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 5}},\n",
    "    patterns=patterns, \n",
    "):\n",
    "    reset_global_random_seed()\n",
    "    config['duration'] = (1+sum(max(pattern) for pattern in patterns)) * config['timeStep']\n",
    "    src  = Population(Neuron, src_neuron_configs, size=src_size, hasInhibition=False, **config)\n",
    "    dest = Population(Neuron, dest_neuron_configs,size=dest_size,hasInhibition=False, **config)\n",
    "    \n",
    "    connections = Connection.full(src.size, dest.size, J0=10,r0=5, wMax=stdp_config['wMax'], wMin=stdp_config['wMin'])\n",
    "    synaptic_tag = np.ones_like(connections['w']) \n",
    "    modulated = {'c': synaptic_tag, 'tau_c': 5}\n",
    "    dopamin = synaptic_tag.copy()\n",
    "    c0_connections = connections['post'] == 0\n",
    "    c1_connections = connections['post'] == 1\n",
    "\n",
    "    \n",
    "    t = 1\n",
    "    many2many_temoral_returned_result = { 'initialW': connections['w'].copy(), 'patterns':[] }\n",
    "    dw_dt = []\n",
    "    dest.transmit_spike()\n",
    "    for pattern in patterns:\n",
    "        dest.transmit_spike(decay=0.9)\n",
    "        \n",
    "        plot_hash = 10 if np.all(pattern-first_pattern==0) else 11 if np.all(pattern-second_pattern==0) else np.argmax(pattern) \n",
    "        many2many_temoral_returned_result['patterns'].append((t,plot_hash))\n",
    "        \n",
    "        for nth in range(int(max(pattern))):\n",
    "            step(src, dest, connections, 'pre', t, get_I(pattern, nth+1), stdp_config, modulated, dopamin, dw_dt)\n",
    "            t += 1\n",
    "\n",
    "\n",
    "        step(src, dest, connections, 'post',  t-1, \n",
    "             get_pattern_effect(pattern, src, dest, connections, t-1), \n",
    "             stdp_config, modulated, dopamin, dw_dt)\n",
    "\n",
    "        last2Spikes = dw_dt[-2:]\n",
    "        reward_punishment(last2Spikes, dopamin, pattern_type(pattern), c0_connections, c1_connections)\n",
    "\n",
    "    many2many_temoral_returned_result['finalW'] = connections['w'].copy()\n",
    "    many2many_temoral_returned_result['dw_dt'] = dw_dt\n",
    "    return many2many_temoral_returned_result\n",
    "\n",
    "dic = many2many(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weights Update ===\n",
      "initial: 1.14  1.64  0.0   2.39  0.0   3.64  0.0   0.0   3.07  0.17  0.0   1.19  2.83  1.98  0.0   0.84  2.23  0.12  3.65  0.42 \n",
      "final  : 1.21  1.74  0.06  2.48  0.04  3.68  0.07  0.0   3.12  0.25  0.07  1.3   2.87  2.06  0.07  0.93  2.28  0.21  3.7   0.5  \n",
      "diff   : 0.06  0.1   0.06  0.09  0.04  0.04  0.07  0.0   0.05  0.08  0.07  0.11  0.05  0.08  0.07  0.1   0.06  0.09  0.05  0.08 \n"
     ]
    }
   ],
   "source": [
    "def print_diff(A,B, title=None):\n",
    "    if title is not None:\n",
    "        print(f\"=== {title} ===\")\n",
    "    print(\"initial:\", \" \".join(map(lambda x: f\"{x:<5}\", np.round(A,2))))\n",
    "    print(\"final  :\" , \" \".join(map(lambda x: f\"{x:<5}\", np.round(B,2))))\n",
    "    print(\"diff   :\" , \" \".join(map(lambda x: f\"{x:<5}\", np.round(B-A,2))))\n",
    "    \n",
    "print_diff(dic['initialW'], dic['finalW'], 'Weights Update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_post_spikes(dw_dt, title, xlabel='t', ylabel='spikes'):\n",
    "    pre_spiked, post_spiked = {'t':[], 's':[]}, {'t':[], 's':[]}\n",
    "    for dw_dt_instance in dw_dt:\n",
    "        if dw_dt_instance[4]=='PRE':\n",
    "            pre_spiked['t'].append(dw_dt_instance[0])\n",
    "            pre_spiked['s'].append(np.where(dw_dt_instance[-1], np.arange(1,11,1), -1).tolist())\n",
    "        else:\n",
    "            post_spiked['t'].append(dw_dt_instance[0])\n",
    "            post_spiked['s'].append(np.where(dw_dt_instance[-1], np.arange(1,3,1), -1).tolist())\n",
    "\n",
    "    pre_spiked['s'] = np.array(pre_spiked['s'])\n",
    "    pre_spiked['t'] = np.array(pre_spiked['t'])\n",
    "\n",
    "    post_spiked['s'] = np.array(post_spiked['s'])\n",
    "    post_spiked['t'] = np.array(post_spiked['t'])\n",
    "    # ----------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10, 5))\n",
    "    if (pre_spiked['s'].size != 0):\n",
    "        for i in range(10):\n",
    "            spike_plotter(pre_spiked['t'], pre_spiked['s'][:, i], \n",
    "                          xlabel=xlabel, ylabel=ylabel, \n",
    "                          title=f\"{title} (PRE-Synaptics)\",\n",
    "                          ax=axes[0])\n",
    "    \n",
    "    if (post_spiked['s'].size != 0):\n",
    "        for i in range(2):\n",
    "            spike_plotter(post_spiked['t'], post_spiked['s'][:, i], \n",
    "                          xlabel=xlabel, ylabel=ylabel, \n",
    "                          title=f\"{title} (POST-Synaptics)\",\n",
    "                          ax=axes[1])\n",
    "\n",
    "    fig.tight_layout(pad=2.0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "x= np.array(dic['patterns'])\n",
    "c = x[:,1].copy()\n",
    "c[c<10] = 0\n",
    "c[c==10]=14\n",
    "c[c==11]=12\n",
    "spike_plotter(x[:,0], x[:,1], \n",
    "              xlabel='t', ylabel='patterns', \n",
    "              title=\"showed pattern duo times for neurons (10 for first & 11 for second pattern)\", \n",
    "              c=c, ax=ax)\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "pre_post_spikes(dic['dw_dt'], title=\"synaptic nueron spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Showed pattern duo times for neurons](https://i.stack.imgur.com/Zh3Jh.png)\n",
    "![PRE-Synaptics and POST-synaptics plots](https://i.stack.imgur.com/KgOfU.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dopamin = np.array([d[-2] for d in dic['dw_dt']])\n",
    "_, ax = plt.subplots(1,1)\n",
    "plt.plot(dopamin)\n",
    "ax.set(xlabel='time', ylabel='dopamin', title='dopamin level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![domain level](https://i.stack.imgur.com/Xooki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2Ô∏è‚É£) Second Question ‚ùì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pattern  = np.array((2, 1, 0, 0, 0, 0, 0, 0, 1, 2))\n",
    "second_pattern = np.array((2, 1, 2, 1, 3, 1, 2, 1, 2, 1))\n",
    "\n",
    "size = {\"first\": 200, \"second\": 190, \"start\": 15, \"middle\": 200, \"end\": 0}\n",
    "learnable_patterns = [first_pattern for i in range(size['first'])]\n",
    "learnable_patterns.extend([second_pattern for i in range(size['second'])])\n",
    "learnable_patterns.extend([random_pattern() for i in range(size['middle'])])\n",
    "random.shuffle(learnable_patterns)\n",
    "patterns = [random_pattern() for i in range(size['start'])]\n",
    "patterns.extend(learnable_patterns)\n",
    "patterns.extend([random_pattern() for i in range(size['end'])])\n",
    "del learnable_patterns\n",
    "\n",
    "def many2many2one(  \n",
    "    srce_size,\n",
    "    srci_size, \n",
    "    dest_size, \n",
    "    config = {'timeStep': 0.01},\n",
    "    stdp_config_ei = {'wMax': 50, 'wMin': 0, 'taup': 8,'taun': 6, 'etap': 0.4, 'etan': 0.5},\n",
    "    stdp_config_ed = {'wMax': 15, 'wMin': 0, 'taup': 8,'taun': 6, 'etap': 0.04, 'etan': 0.05},\n",
    "    stdp_config_id = {'wMax': 20, 'wMin': 0, 'taup': 8,'taun': 6, 'etap': 0.04, 'etan': 0.05},\n",
    "\n",
    "    srce_neuron_configs = {'tau': 5, 'resistor': 5,'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 2}},\n",
    "    srci_neuron_configs = {'tau': 5, 'resistor': 5,'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 1}},\n",
    "    dest_neuron_configs = {'tau': 5, 'resistor': 5, 'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 5}},\n",
    "    patterns=patterns, \n",
    "):\n",
    "    reset_global_random_seed()\n",
    "    config['duration'] = (1+sum(max(pattern) for pattern in patterns)) * config['timeStep']\n",
    "    srce  = Population(Neuron, srce_neuron_configs, size=srce_size, hasInhibition=False, **config)\n",
    "    srci  = Population(Neuron, srci_neuron_configs, size=srci_size, hasInhibition=True, **config)\n",
    "    dest  = Population(Neuron, dest_neuron_configs, size=dest_size, hasInhibition=False, **config)\n",
    "    \n",
    "    connections_ei = Connection.full(srce.size, srci.size, J0=10, r0=5, wMax=stdp_config_ei['wMax'], wMin=stdp_config_ei['wMin'])\n",
    "    connections_ed = Connection.full(srce.size, dest.size, J0=10, r0=5, wMax=stdp_config_ed['wMax'], wMin=stdp_config_ed['wMin'])\n",
    "    connections_id = Connection.full(srci.size, dest.size, J0=10, r0=5, wMax=stdp_config_id['wMax'], wMin=stdp_config_id['wMin'])\n",
    "\n",
    "    \"\"\"\n",
    "        ellipsis = t, d => u[t-d]-u_rest\n",
    "        W*ellipsis(curretnTime, delay)   \n",
    "    \"\"\"\n",
    "    t = 1\n",
    "    many2many_temoral_returned_result = { \n",
    "        'initialWEI': connections_ei['w'].copy(), \n",
    "        'initialWED': connections_ed['w'].copy(), \n",
    "        'initialWID': connections_id['w'].copy(), \n",
    "        'patterns':[] \n",
    "    }\n",
    "    dw_dt_ei, dw_dt_id, dw_dt_ed  = [], [], []\n",
    "    \n",
    "    dest.transmit_spike()\n",
    "    for pattern in patterns:\n",
    "        dest.transmit_spike(decay=0.9)\n",
    "        \n",
    "        plot_hash = 10 if np.all(pattern-first_pattern==0) else 11 if np.all(pattern-second_pattern==0) else np.argmax(pattern) \n",
    "        many2many_temoral_returned_result['patterns'].append((t,plot_hash))\n",
    "        \n",
    "        for nth in range(int(max(pattern))):\n",
    "            step(srce, srci, connections_ei, 'pre', t, get_I(pattern, nth+1), stdp_config_ei, dw_dt_ei, delay=0)\n",
    "            step(srci, dest, connections_id, 'pre', t, get_pattern_effect(pattern, srce, srci, connections_ei, t), \n",
    "                 stdp_config_id, dw_dt_id, delay=0)\n",
    "            step(srce, dest, connections_ed, 'pre', t, get_I(pattern, nth+1), stdp_config_ed, dw_dt_ed, delay=1)\n",
    "            t += 1\n",
    "\n",
    "\n",
    "        # post excititory -> inhibitory\n",
    "        step(srce, srci, connections_ei, 'post',  t-1, \n",
    "             get_pattern_effect(pattern, srce, srci, connections_ei, t-1), stdp_config_ei, dw_dt_ei, delay=0)\n",
    "        # post excititory -> destination\n",
    "        step(srce, dest, connections_ed, 'post',  t-1, \n",
    "             10.0 + get_pattern_effect(pattern, srce, dest, connections_ed, t-1),\n",
    "             stdp_config_ed, dw_dt_ed, delay=1)\n",
    "        # post inhibitory -> destination\n",
    "        step(srci, dest, connections_id, 'post',  t-1, 10.0, stdp_config_id, dw_dt_id, delay=0)\n",
    "\n",
    "        \n",
    "    many2many_temoral_returned_result['finalWEI'] = connections_ei['w'].copy()\n",
    "    many2many_temoral_returned_result['finalWED'] = connections_ed['w'].copy()\n",
    "    many2many_temoral_returned_result['finalWID'] = connections_id['w'].copy()\n",
    "\n",
    "    many2many_temoral_returned_result['dw_dt_ed'] = dw_dt_ed\n",
    "    many2many_temoral_returned_result['dw_dt_ei'] = dw_dt_ei\n",
    "    many2many_temoral_returned_result['dw_dt_id'] = dw_dt_id\n",
    "\n",
    "    return many2many_temoral_returned_result\n",
    "\n",
    "dic = many2many2one(10,1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weights Update for excititory -> inhibitory ===\n",
      "initial: 2.39  0.0   3.64  0.0   0.0   3.07  0.17  0.0   1.19  2.83 \n",
      "final  : 31.2  31.17 37.46 42.89 42.93 42.88 42.91 42.89 31.14 31.19\n",
      "diff   : 28.8  31.17 33.82 42.89 42.93 39.81 42.73 42.89 29.95 28.36\n",
      "=== Weights Update for excititory -> destination ===\n",
      "initial: 1.98  0.0   0.84  2.23  0.12  3.65  0.42  1.75  0.07  0.0   0.6   3.83  0.98  0.0   0.0   1.41  3.04  4.88  0.0   3.0  \n",
      "final  : 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "diff   : -1.98 0.0   -0.84 -2.23 -0.12 -3.65 -0.42 -1.75 -0.07 0.0   -0.6  -3.83 -0.98 0.0   0.0   -1.41 -3.04 -4.88 0.0   -3.0 \n",
      "=== Weights Update for inhibitory -> destination ===\n",
      "initial: 0.92  9.66 \n",
      "final  : 4.74  4.74 \n",
      "diff   : 3.82  -4.92\n"
     ]
    }
   ],
   "source": [
    "print_diff(dic['initialWEI'], dic['finalWEI'], 'Weights Update for excititory -> inhibitory')\n",
    "print_diff(dic['initialWED'], dic['finalWED'], 'Weights Update for excititory -> destination')\n",
    "print_diff(dic['initialWID'], dic['finalWID'], 'Weights Update for inhibitory -> destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "x= np.array(dic['patterns'])\n",
    "c = x[:,1].copy()\n",
    "c[c<10] = 0\n",
    "c[c==10]=14\n",
    "c[c==11]=12\n",
    "spike_plotter(x[:,0], x[:,1], \n",
    "              xlabel='t', ylabel='patterns', \n",
    "              title=\"showed pattern duo times for neurons (10 for first & 11 for second pattern)\", \n",
    "              c=c, ax=ax)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "pre_post_spikes(dic['dw_dt_ed'], title=\"exicitory to destination connection\")\n",
    "pre_post_spikes(dic['dw_dt_id'], title=\"inhibitory to destination connection\")\n",
    "pre_post_spikes(dic['dw_dt_ei'], title=\"exicitory to inhobotory connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![showed pattern duo times for neurons](https://i.stack.imgur.com/fOS2p.png)\n",
    "\n",
    "![exicitory to destination connection](https://i.stack.imgur.com/KZItZ.png)\n",
    "![inhibitory to destination connection](https://i.stack.imgur.com/Rl0s8.png)\n",
    "![exicitory to inhobotory connection](https://i.stack.imgur.com/snOAg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (0.1, 0.25, 0.5)[0]\n",
    "n = (2, 5, 10, 20)[3]\n",
    "\n",
    "def unique_pattern(size, n):\n",
    "    patterns = []\n",
    "    for _ in range(n):\n",
    "        rand = np.random.random(size)\n",
    "        res  = np.ones_like(rand)\n",
    "        res[rand < 0.3] = 0\n",
    "        res[rand >= 0.6] = 2\n",
    "        patterns.append(res)\n",
    "    return patterns\n",
    "\n",
    "config = {'timeStep': 0.01}\n",
    "config['duration'] = (1+sum(max(pattern) for pattern in patterns)) * config['timeStep']\n",
    "ecx_neuron_configs = {'tau': 5, 'resistor': 5,'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 2}}\n",
    "inh_neuron_configs = {'tau': 5, 'resistor': 5,'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 2}}\n",
    "stdp_config = {'wMax': 50, 'wMin': 0, 'taup': 8,'taun': 6, 'etap': 0.4, 'etan': 0.5}\n",
    "\n",
    "exc = Population(Neuron, ecx_neuron_configs, size=800, hasInhibition=False, **config)\n",
    "inh = Population(Neuron, inh_neuron_configs, size=200, hasInhibition=False, **config)\n",
    "\n",
    "connections = Connection.fixedNPP(exc.size, inh.size, p=p, J0=10, r0=5, wMax=stdp_config['wMax'], wMin=stdp_config['wMin'])\n",
    "connections = connections[connections['pre'][np.random.choice(connections['pre'], 50)]]\n",
    "synaptic_tag = np.ones_like(connections['w']) \n",
    "modulated = {'c': synaptic_tag, 'tau_c': 5}\n",
    "dopamin = synaptic_tag.copy()\n",
    "ci_connections = [connections['post'] == i for i in range(n)]\n",
    "\n",
    "\n",
    "#     stdp_config_ed = {'wMax': 15, 'wMin': 0, 'taup': 8,'taun': 6, 'etap': 0.04, 'etan': 0.05},\n",
    "#     stdp_config_id = {'wMax': 20, 'wMin': 0, 'taup': 8,'taun': 6, 'etap': 0.04, 'etan': 0.05},\n",
    "\n",
    "#     srci_neuron_configs = {'tau': 5, 'resistor': 5,'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 1}},\n",
    "#     dest_neuron_configs = {'tau': 5, 'resistor': 5, 'threshold': -60, 'uRest': -75, 'noise': {'mu': 0, 'sigma': 5}},\n",
    "#     patterns=patterns, \n",
    "# ):\n",
    "#     reset_global_random_seed()\n",
    "#     config['duration'] = (1+sum(max(pattern) for pattern in patterns)) * config['timeStep']\n",
    "#     srce  = Population(Neuron, srce_neuron_configs, size=srce_size, hasInhibition=False, **config)\n",
    "#     srci  = Population(Neuron, srci_neuron_configs, size=srci_size, hasInhibition=True, **config)\n",
    "#     dest  = Population(Neuron, dest_neuron_configs, size=dest_size, hasInhibition=False, **config)\n",
    "    \n",
    "#     connections_ei = \n",
    "#     connections_ed = Connection.full(srce.size, dest.size, J0=10, r0=5, wMax=stdp_config_ed['wMax'], wMin=stdp_config_ed['wMin'])\n",
    "#     connections_id = Connection.full(srci.size, dest.size, J0=10, r0=5, wMax=stdp_config_id['wMax'], wMin=stdp_config_id['wMin'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 2, 5, 2, 2, 1, 8, 8, 2, 0, 5, 8, 8, 9, 7, 2, 6, 3, 8, 8, 9,\n",
       "       8, 8, 8, 4, 4, 5, 2, 2, 9, 2, 7, 4, 6, 4, 6, 2, 3, 4, 5, 1, 5, 0,\n",
       "       0, 7, 4, 9, 2, 9], dtype=uint16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connections['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 2., 1., 2., 2., 2., 2., 1., 2., 2., 1., 0., 1., 2., 2., 2., 2.,\n",
       "        1., 1., 2., 0., 1., 0., 0., 2., 1., 1., 1., 2., 0., 0., 0., 2., 0.,\n",
       "        1., 0., 2., 0., 2., 0., 1., 1., 2., 2., 1., 2., 2., 2., 0., 2.]),\n",
       " array([1., 1., 2., 2., 2., 1., 2., 1., 0., 1., 0., 1., 1., 1., 1., 0., 2.,\n",
       "        0., 2., 2., 2., 2., 0., 2., 0., 0., 2., 1., 1., 0., 2., 2., 0., 1.,\n",
       "        0., 1., 2., 0., 2., 0., 1., 1., 1., 2., 0., 2., 0., 2., 0., 1.])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_pattern(50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python388jvsc74a57bd089df480a9e48f4b563af4b1d4fe43b52e15d5b3deeff00f84331d426ccc463e6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
